{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\schae\\.conda\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "<torch.cuda.device object at 0x00000274113AD648>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import re\n",
    "from torchtext.data import Field\n",
    "import spacy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device()) \n",
    "print(torch.cuda.device(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "['im selfish impatient and a little insecure i make mistakes i am out of control and at times hard to handle but if you cant handle me at my worst then you sure as hell dont deserve me at my best'\n",
      " 'youve gotta dance like theres nobody watchinglove like youll never be hurtsing like theres nobody listeningand live like its heaven on earth'\n",
      " 'you know youre in love when you cant fall asleep because reality is finally better than your dreams']\n"
     ]
    }
   ],
   "source": [
    "INPUT = \"data/quotes.csv\"\n",
    "# INPUT = \"/kaggle/input/quotes-500k/quotes.csv\"\n",
    "\n",
    "ds = pd.read_csv(INPUT)\n",
    "\n",
    "ds.head()\n",
    "\n",
    "ds = ds.drop(columns=[\"author\", \"category\"], axis=1)\n",
    "\n",
    "ds = np.array(ds)\n",
    "ds = ds.T[0].astype(str)\n",
    "\n",
    "ds = ds[0:1000]\n",
    "\n",
    "ds = np.char.lower(ds)\n",
    "\n",
    "ds = np.array(list(map(lambda x: re.sub(\"[^a-z0-9\\s]+\", \"\", x), ds)))\n",
    "\n",
    "print(ds.shape)\n",
    "print(ds[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(555,)\n"
     ]
    }
   ],
   "source": [
    "print(ds.shape)\n",
    "\n",
    "MAX_LENGTH = 150\n",
    "\n",
    "def length_check(x):\n",
    "    return len(x) < MAX_LENGTH\n",
    "\n",
    "ds = np.array(list(filter(length_check, ds)))\n",
    "print(ds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fifty_most_common_words = [ \"the\", \"be\", \"of\", \"and\", \"a\", \"to\", \"in\", \"he\", \"have\", \"it\", \"that\", \"for\", \"they\", \"I\", \"with\", \"as\", \"not\", \"on\", \"she\", \"at\", \"by\", \"this\", \"we\", \"you\", \"do\", \"but\", \"from\", \"or\", \"which\", \"one\", \"would\", \"all\", \"will\", \"there\", \"say\", \"who\", \"make\", \"when\", \"can\", \"more\", \"if\", \"no\", \"man\", \"out\", \"other\", \"so\", \"what\", \"time\", \"up\", \"go\"]\n",
    "\n",
    "text = \" \".join(ds)\n",
    "# def remove_fifty_most_common_words_from_text(text):\n",
    "#     for word in fifty_most_common_words:\n",
    "#         text = text.replace(word, \"\")\n",
    "#     return text\n",
    "\n",
    "# text = remove_fifty_most_common_words_from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301\n",
      "['a', 'able', 'about', 'act', 'after', 'again', 'all', 'always', 'am', 'an', 'and', 'another', 'any', 'anything', 'are', 'arm', 'around', 'art', 'as', 'at', 'away', 'back', 'be', 'beautiful', 'because', 'become', 'been', 'being', 'best', 'bet', 'better', 'between', 'body', 'break', 'but', 'by', 'can', 'cannot', 'cant', 'care', 'cause', 'change', 'choice', 'choose', 'close', 'come', 'comes', 'could', 'dark', 'day', 'deep', 'did', 'die', 'do', 'doe', 'does', 'doesnt', 'dont', 'each', 'ear', 'ears', 'eat', 'else', 'end', 'ends', 'enough', 'even', 'ever', 'every', 'everything', 'eye', 'fall', 'fat', 'feel', 'fight', 'find', 'fire', 'for', 'forever', 'forget', 'friend', 'friends', 'from', 'full', 'get', 'give', 'go', 'god', 'going', 'good', 'great', 'had', 'hang', 'happiness', 'happy', 'has', 'hate', 'have', 'he', 'hear']\n",
      "['worth', 'would', 'ye', 'yes', 'you', 'youll', 'your', 'youre', 'yours', 'yourself']\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 20000\n",
    "max_len = 400\n",
    "\n",
    "def letters(input):\n",
    "    valids = []\n",
    "    for character in input:\n",
    "        if character.isalpha():\n",
    "            valids.append(character)\n",
    "    return ''.join(valids)\n",
    "\n",
    "\n",
    "def more_than_once(input):\n",
    "    if len(input) < 1:\n",
    "        return False\n",
    "    return text.count(input) > 6\n",
    "words = text.split(\" \")\n",
    "\n",
    "words = list(set(words))\n",
    "vocab = sorted(words)\n",
    "\n",
    "vocab = list(map(letters, vocab))\n",
    "vocab = list(filter(lambda x: len(x) > 0, vocab))\n",
    "vocab = list(set(vocab))\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "vocab = list(filter(more_than_once, vocab))\n",
    "\n",
    "print(len(vocab))\n",
    "print(vocab[0:100])\n",
    "print(vocab[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['youve', 'gotta', 'dance', 'like', 'theres', 'nobody', 'watchinglove', 'like', 'youll', 'never', 'be', 'hurtsing', 'like', 'theres', 'nobody', 'listeningand', 'live', 'like', 'its', 'heaven', 'on', 'earth'], ['you', 'know', 'youre', 'in', 'love', 'when', 'you', 'cant', 'fall', 'asleep', 'because', 'reality', 'is', 'finally', 'better', 'than', 'your', 'dreams'], ['a', 'friend', 'is', 'someone', 'who', 'knows', 'all', 'about', 'you', 'and', 'still', 'loves', 'you']]\n",
      "[('you', 379), ('love', 363), ('the', 356), ('to', 269), ('i', 227), ('is', 226), ('a', 198), ('and', 184), ('of', 178), ('in', 141), ('it', 140), ('that', 118), ('be', 98), ('for', 80), ('not', 78), ('are', 76), ('with', 75), ('my', 71), ('if', 70), ('when', 62), ('me', 62), ('your', 61), ('we', 61), ('but', 61), ('one', 60), ('all', 51), ('can', 51), ('have', 51), ('heart', 49), ('as', 48), ('someone', 46), ('its', 45), ('what', 45), ('like', 43), ('was', 43), ('who', 42), ('never', 41), ('loved', 41), ('there', 40), ('dont', 40), ('than', 39), ('only', 38), ('he', 37), ('just', 35), ('so', 35), ('do', 34), ('life', 34), ('know', 33), ('they', 33), ('more', 33), ('because', 32), ('them', 32), ('people', 32), ('no', 30), ('on', 29), ('youre', 29), ('time', 28), ('this', 28), ('make', 27), ('will', 27), ('at', 26), ('her', 26), ('or', 25), ('would', 25), ('things', 25), ('always', 25), ('how', 25), ('much', 25), ('world', 25), ('think', 24), ('by', 23), ('ever', 23), ('us', 23), ('go', 23), ('were', 23), ('want', 23), ('cant', 22), ('about', 22), ('way', 22), ('man', 22), ('could', 22), ('thing', 22), ('find', 22), ('his', 22), ('too', 21), ('being', 20), ('person', 20), ('him', 20), ('im', 20), ('even', 18), ('better', 17), ('then', 17), ('into', 17), ('sometimes', 17), ('without', 17), ('loves', 16), ('up', 16), ('something', 16), ('am', 16), ('she', 16), ('makes', 15), ('true', 15), ('those', 15), ('thats', 15), ('feel', 14), ('woman', 14), ('nothing', 14), ('from', 14), ('said', 14), ('let', 14), ('over', 14), ('fall', 13), ('loving', 13), ('real', 13), ('get', 13), ('any', 13), ('soul', 13), ('yourself', 13), ('best', 13), ('which', 12), ('been', 12), ('where', 12), ('whole', 12), ('day', 12), ('away', 12), ('other', 12), ('back', 12), ('out', 11), ('an', 11), ('reason', 11), ('everything', 11), ('right', 11), ('our', 11), ('beautiful', 11), ('does', 11), ('end', 11), ('god', 11), ('most', 11), ('give', 11), ('cannot', 10), ('another', 10), ('had', 10), ('has', 10), ('good', 10), ('say', 10), ('matter', 10), ('each', 10), ('together', 10), ('care', 10), ('after', 10), ('their', 10), ('ill', 10), ('youll', 9), ('live', 9), ('still', 9), ('moment', 9), ('anything', 9), ('really', 9), ('forever', 9), ('same', 9), ('around', 9), ('happy', 9), ('two', 9), ('isnt', 9), ('long', 9), ('some', 9), ('else', 9), ('doesnt', 9), ('kiss', 9), ('little', 9), ('hurt', 9), ('part', 9), ('again', 9), ('place', 8), ('happiness', 8), ('see', 8), ('die', 8), ('must', 8), ('hold', 8), ('myself', 8), ('made', 8), ('looking', 8), ('going', 8), ('remember', 8), ('take', 8), ('should', 8), ('human', 8), ('change', 8), ('need', 8), ('choose', 8), ('theres', 7), ('hate', 7), ('trust', 7), ('very', 7), ('between', 7), ('look', 7), ('hope', 7), ('night', 7), ('may', 7), ('fire', 7), ('somebody', 7), ('stars', 7), ('power', 7), ('peace', 7), ('close', 7), ('wish', 7), ('well', 7), ('enough', 7), ('holding', 7), ('ones', 7), ('did', 7), ('nobody', 6), ('friend', 6), ('light', 6), ('fell', 6), ('wrong', 6), ('own', 6), ('eyes', 6), ('theyre', 6), ('every', 6), ('universe', 6), ('rather', 6), ('having', 6), ('shall', 6), ('wasnt', 6), ('great', 6), ('truly', 6), ('romance', 6), ('hard', 6), ('last', 6), ('now', 6), ('greatest', 6), ('stop', 6), ('against', 6), ('takes', 6), ('already', 6), ('far', 6), ('ive', 6), ('left', 6), ('sweet', 6), ('less', 6), ('come', 6), ('until', 6), ('fight', 6), ('choice', 6), ('oh', 6), ('once', 5), ('gives', 5), ('tell', 5), ('perfect', 5), ('talk', 5), ('lost', 5), ('certain', 5), ('secret', 5), ('become', 5), ('boy', 5), ('wanted', 5), ('means', 5), ('mine', 5), ('also', 5), ('first', 5), ('down', 5), ('new', 5), ('sky', 5), ('cry', 5), ('many', 5), ('anymore', 5), ('felt', 5), ('years', 5), ('both', 5), ('worth', 5), ('off', 5), ('ourselves', 5), ('body', 5), ('keep', 5), ('why', 5), ('lovers', 5), ('fate', 5), ('youve', 4), ('deserve', 4), ('deeply', 4), ('friends', 4), ('smiling', 4), ('thought', 4), ('walk', 4), ('imagination', 4), ('dark', 4), ('feelings', 4), ('help', 4), ('laughter', 4), ('looked', 4), ('nice', 4), ('supposed', 4), ('meant', 4), ('three', 4), ('alone', 4), ('home', 4), ('madness', 4), ('try', 4), ('goes', 4), ('leave', 4), ('return', 4), ('yours', 4), ('smile', 4), ('hearts', 4), ('beauty', 4), ('lips', 4), ('able', 4), ('lose', 4), ('affection', 4), ('words', 4), ('write', 4), ('children', 4), ('letting', 4), ('mean', 4), ('forget', 4), ('stay', 4), ('difference', 4), ('anybody', 4), ('rose', 4), ('knowing', 4), ('full', 4), ('understand', 4), ('side', 4), ('easy', 4), ('word', 4), ('respect', 4), ('reasons', 4), ('hand', 4), ('alive', 4), ('though', 4), ('gone', 4), ('comes', 4), ('destroy', 4), ('lot', 4), ('sure', 4), ('inside', 4), ('blood', 4), ('miss', 4), ('men', 4), ('name', 4), ('except', 4), ('maybe', 4), ('story', 4), ('sacrifice', 4), ('truth', 4), ('heaven', 3), ('earth', 3), ('asleep', 3), ('finally', 3), ('knows', 3), ('darkness', 3), ('drive', 3), ('accept', 3), ('friendship', 3), ('happens', 3), ('laugh', 3), ('spite', 3), ('fact', 3), ('nature', 3), ('lover', 3), ('wont', 3), ('entire', 3), ('anyone', 3), ('girl', 3), ('sign', 3), ('broken', 3), ('whom', 3), ('art', 3), ('hes', 3), ('whats', 3), ('short', 3), ('forgetting', 3), ('changes', 3), ('discovered', 3), ('women', 3), ('whether', 3), ('burn', 3), ('house', 3), ('sight', 3), ('these', 3), ('thinking', 3), ('hurts', 3), ('war', 3), ('womans', 3), ('greater', 3), ('promise', 3), ('works', 3), ('pretty', 3), ('kid', 3), ('falling', 3), ('sleep', 3), ('found', 3), ('leaving', 3), ('breathe', 3), ('turn', 3), ('burst', 3), ('moon', 3), ('id', 3), ('here', 3), ('getting', 3), ('arms', 3), ('gift', 3), ('front', 3), ('knew', 3), ('times', 3), ('absence', 3), ('stand', 3), ('seen', 3), ('relationship', 3), ('telling', 3), ('show', 3), ('kind', 3), ('sun', 3), ('throw', 3), ('hear', 3), ('voice', 3), ('belong', 3), ('before', 3), ('lovely', 3), ('course', 3), ('missing', 3), ('such', 3), ('put', 3), ('marry', 3), ('humans', 3), ('swear', 3), ('stronger', 3), ('wants', 3), ('tired', 3), ('done', 3), ('fear', 3), ('conquer', 3), ('standing', 3), ('ye', 3), ('existence', 3), ('toward', 3), ('hundred', 3), ('awake', 3), ('soon', 3), ('whispered', 3), ('kissed', 3), ('damn', 3), ('ends', 3), ('monster', 3), ('didnt', 3), ('evil', 3), ('watchinglove', 2), ('dreams', 2), ('hated', 2), ('read', 2), ('lack', 2), ('while', 2), ('courage', 2), ('heartbeat', 2), ('single', 2), ('flashing', 2), ('wind', 2), ('space', 2), ('flower', 2), ('through', 2), ('garden', 2), ('different', 2), ('safe', 2), ('big', 2), ('adventure', 2), ('jumps', 2), ('worse', 2), ('shadow', 2), ('someones', 2), ('complete', 2), ('strive', 2), ('becomes', 2), ('vain', 2), ('dream', 2), ('hates', 2), ('upon', 2), ('question', 2), ('bear', 2), ('needs', 2), ('follow', 2), ('thou', 2), ('feeling', 2), ('play', 2), ('equal', 2), ('direction', 2), ('everyone', 2), ('remain', 2), ('sea', 2), ('seek', 2), ('unrequited', 2), ('meaning', 2), ('today', 2), ('breaking', 2), ('realize', 2), ('tragedy', 2), ('sin', 2), ('caution', 2), ('destiny', 2), ('reach', 2), ('touch', 2), ('hands', 2), ('waste', 2), ('instead', 2), ('weep', 2), ('hell', 2), ('process', 2), ('special', 2), ('kill', 2), ('enemies', 2), ('air', 2), ('necessary', 2), ('measure', 2), ('number', 2), ('form', 2), ('common', 2), ('presence', 2), ('spell', 2), ('saddest', 2), ('act', 2), ('flames', 2), ('seeing', 2), ('realizing', 2), ('control', 2), ('lives', 2), ('lining', 2), ('actually', 2), ('tears', 2), ('pure', 2), ('music', 2), ('making', 2), ('spent', 2), ('wasted', 2), ('page', 2), ('book', 2), ('affair', 2), ('oneself', 2), ('beginning', 2), ('box', 2), ('memory', 2), ('often', 2), ('saw', 2), ('ways', 2), ('apart', 2), ('realized', 2), ('sorrow', 2), ('learn', 2), ('larger', 2), ('gets', 2), ('pain', 2), ('empty', 2), ('break', 2), ('catch', 2), ('harm', 2), ('family', 2), ('eat', 2), ('might', 2), ('depth', 2), ('separation', 2), ('wouldnt', 2), ('wonder', 2), ('thee', 2), ('begins', 2), ('fully', 2), ('fighting', 2), ('point', 2), ('usually', 2), ('staying', 2), ('sunlight', 2), ('says', 2), ('lights', 2), ('set', 2), ('among', 2), ('forgotten', 2), ('next', 2), ('prince', 2), ('run', 2), ('strange', 2), ('speaking', 2), ('showed', 2), ('sex', 2), ('compassion', 2), ('born', 2), ('learned', 2), ('attention', 2), ('giving', 2), ('function', 2), ('mother', 2), ('death', 2), ('heal', 2), ('risk', 2), ('future', 2), ('worry', 2), ('okay', 2), ('hang', 2), ('expecting', 2), ('bird', 2), ('quietly', 2), ('difficult', 2), ('beloved', 2), ('dies', 2), ('behind', 2), ('old', 2), ('moments', 2), ('feels', 2), ('submission', 2), ('mankind', 2), ('despite', 2), ('step', 2), ('edges', 2), ('happily', 2), ('catalogue', 2), ('distance', 2), ('idea', 2), ('ought', 2), ('unless', 2), ('mothers', 2), ('survive', 2), ('creature', 2), ('dear', 2), ('trying', 2), ('song', 2), ('heard', 2), ('called', 2), ('spaces', 2), ('walls', 2), ('worst', 2), ('selves', 2), ('grief', 2), ('jealous', 2), ('others', 2), ('wait', 2), ('rest', 2), ('dead', 2), ('got', 2), ('please', 2), ('past', 2), ('gods', 2), ('wishes', 2), ('search', 2), ('themselves', 2), ('seemed', 2), ('strongest', 2), ('anna', 2), ('karenina', 2), ('mind', 2), ('gather', 2), ('pieces', 2), ('order', 2), ('bright', 2), ('sick', 2), ('healthy', 2), ('1', 2), ('met', 2), ('humblest', 2), ('room', 2), ('eye', 2), ('hello', 2), ('longer', 2), ('loyalty', 2), ('memories', 2), ('closed', 2), ('heartbreak', 2), ('under', 2), ('wonderful', 2), ('forgive', 2), ('shines', 2), ('believe', 2), ('animal', 2), ('animals', 2), ('choices', 2), ('goodbye', 2), ('happened', 2), ('lines', 2), ('endure', 2), ('second', 2), ('least', 2), ('fault', 2), ('save', 2), ('lie', 2), ('wins', 2), ('threw', 2), ('deep', 2), ('meaningless', 2), ('gotta', 1), ('dance', 1), ('hurtsing', 1), ('listeningand', 1), ('reality', 1), ('slowly', 1), ('unhappy', 1), ('marriages', 1), ('few', 1), ('none', 1), ('strength', 1), ('accidentally', 1), ('throbbing', 1), ('condition', 1), ('essential', 1), ('reali', 1), ('looks', 1), ('mindand', 1), ('therefore', 1), ('winged', 1), ('cupid', 1), ('painted', 1), ('blind', 1), ('notion', 1), ('halves', 1), ('thrill', 1), ('kissing', 1), ('forehead', 1), ('staring', 1), ('youi', 1), ('comfortable', 1), ('awfully', 1), ('ladys', 1), ('rapid', 1), ('admiration', 1), ('matrimony', 1), ('tis', 1), ('needing', 1), ('putting', 1), ('bad', 1), ('qualities', 1), ('somehow', 1), ('needed', 1), ('struggled', 1), ('repressed', 1), ('allow', 1), ('ardently', 1), ('admire', 1), ('conspired', 1), ('treats', 1), ('ordinary', 1), ('dignified', 1), ('spend', 1), ('answering', 1), ('measured', 1), ('poets', 1), ('tried', 1), ('convinced', 1), ('require', 1), ('eleanor', 1), ('whatever', 1), ('souls', 1), ('decided', 1), ('stick', 1), ('lovehate', 1), ('burden', 1), ('isolated', 1), ('above', 1), ('thine', 1), ('self', 1), ('canst', 1), ('false', 1), ('essence', 1), ('uncertainty', 1), ('daddy', 1), ('endthe', 1), ('takeis', 1), ('consist', 1), ('gazing', 1), ('outward', 1), ('warm', 1), ('hearth', 1), ('forgets', 1), ('sit', 1), ('stone', 1), ('bread', 1), ('remade', 1), ('faith', 1), ('herthen', 1), ('careful', 1), ('itll', 1), ('twist', 1), ('brain', 1), ('prettyi', 1), ('beautifulbeautifulyou', 1), ('fish', 1), ('begin', 1), ('seem', 1), ('battle', 1), ('growing', 1), ('hidden', 1), ('declarations', 1), ('amuse', 1), ('especially', 1), ('rare', 1), ('irresistible', 1), ('desire', 1), ('irresistibly', 1), ('desired', 1), ('yesterdayi', 1), ('tomorrowsimply', 1), ('overcomes', 1), ('minds', 1), ('heads', 1), ('kinds', 1), ('curious', 1), ('ridiculous', 1), ('sinner', 1), ('forms', 1), ('perhaps', 1), ('fatal', 1), ('discouragement', 1), ('amazing', 1), ('overrated', 1), ('songs', 1), ('fat', 1), ('cake', 1), ('gravitation', 1), ('responsible', 1), ('creating', 1), ('fool', 1), ('stuff', 1), ('maintain', 1), ('suffering', 1), ('unable', 1), ('opened', 1), ('affectionate', 1), ('steady', 1), ('persons', 1), ('ultimate', 1), ('obtained', 1), ('painful', 1), ('losing', 1), ('knowledge', 1), ('coming', 1), ('expect', 1), ('dined', 1), ('inhabits', 1), ('unnoticed', 1), ('seems', 1), ('genuinely', 1), ('shyness', 1), ('unexpected', 1), ('warmth', 1), ('rushes', 1), ('detriment', 1), ('steps', 1), ('spring', 1), ('cherry', 1), ('trees', 1), ('mutual', 1), ('beings', 1), ('exchange', 1), ('fascinated', 1), ('pigletyou', 1), ('ityou', 1), ('pooh', 1), ('tonight', 1), ('linesi', 1), ('shared', 1), ('definition', 1), ('dictionary', 1), ('loveoften', 1), ('deciphered', 1), ('breath', 1), ('consequence', 1), ('askletting', 1), ('gonot', 1), ('liquid', 1), ('withinthe', 1), ('skin', 1), ('sort', 1), ('tender', 1), ('curiosity', 1), ('nightwhen', 1), ('sleeppraise', 1), ('insomniasand', 1), ('touches', 1), ('writes', 1), ('keeps', 1), ('photographs', 1), ('shed', 1), ('weakness', 1), ('conversation', 1), ('lyrics', 1), ('melody', 1), ('replayed', 1), ('stale', 1), ('important', 1), ('fill', 1), ('syllable', 1), ('halfread', 1), ('halffinished', 1), ('lifelong', 1), ('flaws', 1), ('interesting', 1), ('gave', 1), ('took', 1), ('oxygen', 1), ('dying', 1), ('poor', 1), ('aslans', 1), ('aslan', 1), ('lead', 1), ('narnian', 1), ('narnia', 1), ('purpose', 1), ('controlling', 1), ('whoever', 1), ('parent', 1), ('baby', 1), ('awkwardly', 1), ('deal', 1), ('mess', 1), ('smiled', 1), ('deeper', 1), ('holds', 1), ('marks', 1), ('risks', 1), ('degenerating', 1), ('obsession', 1), ('sharing', 1), ('frees', 1), ('weight', 1), ('lifethat', 1), ('invented', 1), ('cover', 1), ('guess', 1), ('judges', 1), ('emotion', 1), ('heals', 1), ('passing', 1), ('waving', 1), ('train', 1), ('tighter', 1), ('eleanors', 1), ('butterfly', 1), ('completely', 1), ('seas', 1), ('backward', 1), ('learning', 1), ('sequestered', 1), ('nooksand', 1), ('serenity', 1), ('books', 1), ('jail', 1), ('500', 1), ('taste', 1), ('peanut', 1), ('butter', 1), ('quite', 1), ('unlikely', 1), ('hour', 1), ('diminishes', 1), ('small', 1), ('increases', 1), ('blows', 1), ('candle', 1), ('fans', 1), ('bonfire', 1), ('count', 1), ('waysi', 1), ('breadth', 1), ('heightmy', 1), ('trip', 1), ('impossible', 1), ('starts', 1), ('anyhow', 1), ('offering', 1), ('border', 1), ('miraculous', 1), ('ceremonious', 1), ('validation', 1), ('inconvenient', 1), ('promised', 1), ('claps', 1), ('moonbeams', 1), ('kissings', 1), ('timethe', 1), ('earthyou', 1), ('owe', 1), ('melookwhat', 1), ('happenswith', 1), ('thatit', 1), ('balance', 1), ('living', 1), ('balancedlife', 1), ('free', 1), ('surprised', 1), ('herpes', 1), ('lad', 1), ('kite', 1), ('bring', 1), ('country', 1), ('sought', 1), ('climbed', 1), ('topmost', 1), ('steeple', 1), ('declared', 1), ('dwell', 1), ('goal', 1), ('picture', 1), ('secretly', 1), ('frogs', 1), ('prefer', 1), ('dawnkaleidoscopic', 1)]\n",
      "tensor([[ 347, 1034,  877,  ...,    1,    1,    1],\n",
      "        [   2,   49,   57,  ...,    1,    1,    1],\n",
      "        [   8,  222,    7,  ...,    1,    1,    1]])\n",
      "1637\n"
     ]
    }
   ],
   "source": [
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "def tokenizer_fn(s): \n",
    "    s = str(s)\n",
    "    return [w.text.lower() for w in nlp(s)]\n",
    "\n",
    "# Define the preprocessing steps for your text fields\n",
    "tokenizer = Field(sequential=True, tokenize=\"basic_english\", lower=True, batch_first=True, fix_length=max_len)\n",
    "\n",
    "# quote = ds[0]\n",
    "# quote = str(quote)\n",
    "\n",
    "# tokenized = tokenizer.tokenize(quote)\n",
    "\n",
    "# print(tokenized)\n",
    "\n",
    "# tokenizer.build_vocab([tokenized])\n",
    "\n",
    "# print(tokenizer.vocab.freqs.most_common(10))\n",
    "# print(len(tokenizer.vocab))\n",
    "\n",
    "# numericalized = tokenizer.process([tokenized])\n",
    "\n",
    "# print(numericalized)\n",
    "\n",
    "def tokenize(text):\n",
    "    text = str(text)\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "dataset = list(map(tokenize, ds))\n",
    "\n",
    "print(dataset[0:3])\n",
    "\n",
    "tokenizer.build_vocab(dataset, max_size=max_tokens)\n",
    "\n",
    "print(tokenizer.vocab.freqs.most_common(1000))\n",
    "\n",
    "numericalized_dataset = tokenizer.process(dataset)\n",
    "\n",
    "print(numericalized_dataset[0:3])\n",
    "\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([ 347, 1034,  877,   35,  208,  232,  673,   35,  174,   39,   14, 1087,\n",
      "          35,  208,  232, 1163,  165,   35,   33,  389,   56,  370,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1]), tensor([1034,  877,   35,  208,  232,  673,   35,  174,   39,   14, 1087,   35,\n",
      "         208,  232, 1163,  165,   35,   33,  389,   56,  370,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1])), (tensor([   2,   49,   57,   11,    3,   22,    2,   79,  115,  353,   52, 1339,\n",
      "           7,  378,   92,   42,   25,  501,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1]), tensor([  49,   57,   11,    3,   22,    2,   79,  115,  353,   52, 1339,    7,\n",
      "         378,   92,   42,   25,  501,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1])), (tensor([  8, 222,   7,  32,  37, 401,  27,  78,   2,   9, 172,  98,   2,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1]), tensor([222,   7,  32,  37, 401,  27,  78,   2,   9, 172,  98,   2,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1]))]\n"
     ]
    }
   ],
   "source": [
    "def split_input_sequence(x):\n",
    "    input_text = x[:-1]\n",
    "    target_text = x[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "# sample = numericalized_dataset[0]\n",
    "\n",
    "# print(sample)\n",
    "\n",
    "# print(split_input_sequence(sample))\n",
    "\n",
    "dataset = list(map(split_input_sequence, numericalized_dataset))\n",
    "\n",
    "# print(dataset.shape)\n",
    "\n",
    "print(dataset[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 399])\n",
      "torch.Size([32, 399])\n"
     ]
    }
   ],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        # Process and return the sample\n",
    "        return sample\n",
    "\n",
    "dataset = MyDataset(dataset)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,\n",
    "                        generator=torch.Generator(device='cuda'),\n",
    "                    )\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch[0].shape)\n",
    "    print(batch[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 399, 1637])\n",
      "torch.Size([32, 399])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [32, 1637], got [32, 399]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27232\\334753628.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\schae\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\schae\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1174\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1176\u001b[1;33m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[0;32m   1177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\schae\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3024\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3025\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3026\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3028\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected target size [32, 1637], got [32, 399]"
     ]
    }
   ],
   "source": [
    "embedding_dim = 1024\n",
    "rnn_hidden_dim = 2048\n",
    "\n",
    "class QGT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QGT, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim=embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, rnn_hidden_dim, num_layers=1)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "model = QGT()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(100):\n",
    "        batch = next(iter(dataloader))\n",
    "        input_text = batch[0]\n",
    "        target_text = batch[1]\n",
    "\n",
    "        output = model(input_text)\n",
    "\n",
    "        print(output.shape)\n",
    "        print(target_text.shape)\n",
    "        loss = criterion(output, target_text)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
